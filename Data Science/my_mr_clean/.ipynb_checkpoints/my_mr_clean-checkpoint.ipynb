{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324fe233-d067-49ce-9df0-920ccff60353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: 1\n",
      "not: 1\n",
      "available: 1\n",
      "or: 1\n",
      "error: 1\n",
      "response: 1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import Counter\n",
    "\n",
    "def get_content(article_name):\n",
    "    \"\"\"\n",
    "    Fetch the content of a Wikipedia article using the Wikipedia API.\n",
    "    \n",
    "    Parameters:\n",
    "    - article_name (str): The name of the article to fetch.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing metadata and article content.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": article_name,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "    }\n",
    "    \n",
    "    # Send the request to the Wikipedia API\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def merge_contents(data, splitters=[\" \", \"\\n\"]):\n",
    "    \"\"\"\n",
    "    Extract, clean, and split the content from the Wikipedia API response using the splitter characters.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (dict): The API response containing article content.\n",
    "    - splitters (list): A list of characters used to split the content into words (default: space and newline).\n",
    "    \n",
    "    Returns:\n",
    "    - str: A cleaned and split string containing the article content.\n",
    "    \"\"\"\n",
    "    # Extracting content from the API response\n",
    "    try:\n",
    "        pages = data['query']['pages']\n",
    "        page_content = next(iter(pages.values()))  # Get the first page from the response\n",
    "        revisions = page_content['revisions']\n",
    "        content = revisions[0]['slots']['main']['content']\n",
    "        \n",
    "        # Clean and simplify the content (remove unnecessary parts)\n",
    "        content = content.replace(\"{{pp-semi-indef}}\", \"\").strip()  # Removing template\n",
    "        content = content.replace(\"\\n\", \" \")  # Replace new lines with spaces to avoid splitting too much\n",
    "        \n",
    "        # Split the content based on the \"splitter\" list\n",
    "        for splitter in splitters:\n",
    "            content = content.replace(splitter, \" \")  # Replace each splitter with a space\n",
    "        \n",
    "        # Return a portion of the content after splitting (for readability)\n",
    "        content = content[:500]  # Limit to the first 500 characters for display\n",
    "        \n",
    "        return content\n",
    "    except KeyError:\n",
    "        return \"Content not available or error in the response.\"\n",
    "\n",
    "def tokenize(content):\n",
    "    \"\"\"\n",
    "    Tokenize the content into individual words (or tokens) based on a set of delimiters.\n",
    "    \n",
    "    Parameters:\n",
    "    - content (str): The content to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tokens (words).\n",
    "    \"\"\"\n",
    "    # Define the delimiters we want to use for tokenization (e.g., spaces, newlines, punctuation)\n",
    "    splitters = [\" \", \"\\n\", \".\", \",\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \";\", \":\", \"-\", \"_\", \"'\", \"\\\"\"]\n",
    "    \n",
    "    # Replace the delimiters with spaces\n",
    "    for splitter in splitters:\n",
    "        content = content.replace(splitter, \" \")\n",
    "    \n",
    "    # Tokenize by splitting the content into words based on spaces\n",
    "    tokens = content.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def lower_collection(collection):\n",
    "    \"\"\"\n",
    "    Convert all tokens in the collection to lowercase.\n",
    "    \n",
    "    Parameters:\n",
    "    - collection (list): A list of tokens (words).\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of tokens in lowercase.\n",
    "    \"\"\"\n",
    "    # Convert each token in the collection to lowercase\n",
    "    return [token.lower() for token in collection]\n",
    "\n",
    "def count_frequency(collection):\n",
    "    \"\"\"\n",
    "    Count the frequency of each token in the collection.\n",
    "    \n",
    "    Parameters:\n",
    "    - collection (list): A list of tokens (words).\n",
    "    \n",
    "    Returns:\n",
    "    - Counter: A dictionary-like object containing token frequencies.\n",
    "    \"\"\"\n",
    "    # Use Counter from collections to count token frequencies\n",
    "    return Counter(collection)\n",
    "\n",
    "def print_most_frequent(frequencies, n):\n",
    "    \"\"\"\n",
    "    Print the `n` most frequent tokens and their counts.\n",
    "    \n",
    "    Parameters:\n",
    "    - frequencies (Counter): A dictionary-like object containing token frequencies.\n",
    "    - n (int): The number of most frequent tokens to display.\n",
    "    \"\"\"\n",
    "    # Get the most common `n` items from the frequency Counter\n",
    "    most_common = frequencies.most_common(n)\n",
    "    \n",
    "    # Print the most frequent tokens and their frequencies\n",
    "    for token, freq in most_common:\n",
    "        print(f\"{token}: {freq}\")\n",
    "\n",
    "def remove_stop_words(words, stop_words):\n",
    "    \"\"\"\n",
    "    Remove stop words from the list of words.\n",
    "    \n",
    "    Parameters:\n",
    "    - words (list): A list of tokens (words).\n",
    "    - stop_words (list): A list of stop words to be removed.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A filtered list of tokens with stop words removed.\n",
    "    \"\"\"\n",
    "    # Filter out any word that is in the stop_words list\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "# Main part of the script\n",
    "article_name = \"Ozone_layer\"\n",
    "stop_words = [\"the\", \"a\", \"of\", \"to\", \"in\", \"about\", \"and\", \"that\", \"for\", \"is\", \"on\", \"it\", \"as\", \"with\", \"an\", \"at\", \"by\"]\n",
    "\n",
    "data = get_content(article_name)\n",
    "merged_content = merge_contents(data)\n",
    "\n",
    "# Tokenize the merged content\n",
    "collection = tokenize(merged_content)\n",
    "\n",
    "# Convert all tokens to lowercase\n",
    "lowered_collection = lower_collection(collection)\n",
    "\n",
    "# Remove stop words from the collection\n",
    "filtered_collection = remove_stop_words(lowered_collection, stop_words)\n",
    "\n",
    "# Count the frequency of each token in the filtered collection\n",
    "frequencies = count_frequency(filtered_collection)\n",
    "\n",
    "# Print the 10 most frequent tokens\n",
    "print_most_frequent(frequencies, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48660c67-0eda-4156-9297-138fd8eabc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
